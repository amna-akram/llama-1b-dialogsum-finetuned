{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:05:45.230639Z","iopub.execute_input":"2025-02-23T00:05:45.230956Z","iopub.status.idle":"2025-02-23T00:05:45.618656Z","shell.execute_reply.started":"2025-02-23T00:05:45.230922Z","shell.execute_reply":"2025-02-23T00:05:45.617707Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# workaround to resolve version conflicts\n!pip install --use-deprecated=legacy-resolver -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:06:50.718833Z","iopub.execute_input":"2025-02-23T00:06:50.719186Z","iopub.status.idle":"2025-02-23T00:06:55.738872Z","shell.execute_reply.started":"2025-02-23T00:06:50.719159Z","shell.execute_reply":"2025-02-23T00:06:55.737980Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/einops/\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import bitsandbytes\nimport transformers\nimport peft\nimport accelerate\nimport datasets\nimport scipy\nimport einops\nimport evaluate\nimport trl\nimport rouge_score\n\nprint(\"BitsAndBytes version:\", bitsandbytes.__version__)\nprint(\"Transformers version:\", transformers.__version__)\nprint(\"PEFT version:\", peft.__version__)\nprint(\"Accelerate version:\", accelerate.__version__)\nprint(\"Datasets version:\", datasets.__version__)\nprint(\"Scipy version:\", scipy.__version__)\nprint(\"Einops version:\", einops.__version__)\nprint(\"Evaluate version:\", evaluate.__version__)\nprint(\"TRL version:\", trl.__version__)\n# commenting out the line below as rouge_score doesn't have version attribute\n# print(\"Rouge Score version:\", rouge_score.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:06:58.741450Z","iopub.execute_input":"2025-02-23T00:06:58.741773Z","iopub.status.idle":"2025-02-23T00:07:21.184308Z","shell.execute_reply.started":"2025-02-23T00:06:58.741748Z","shell.execute_reply":"2025-02-23T00:07:21.183472Z"}},"outputs":[{"name":"stdout","text":"BitsAndBytes version: 0.45.2\nTransformers version: 4.49.0\nPEFT version: 0.14.0\nAccelerate version: 1.4.0\nDatasets version: 3.3.2\nScipy version: 1.15.2\nEinops version: 0.8.1\nEvaluate version: 0.4.3\nTRL version: 0.15.1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# to get version\n!pip show rouge-score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:07:23.966569Z","iopub.execute_input":"2025-02-23T00:07:23.967225Z","iopub.status.idle":"2025-02-23T00:07:27.150912Z","shell.execute_reply.started":"2025-02-23T00:07:23.967192Z","shell.execute_reply":"2025-02-23T00:07:27.149693Z"}},"outputs":[{"name":"stdout","text":"Name: rouge_score\nVersion: 0.1.2\nSummary: Pure python implementation of ROUGE-1.5.5.\nHome-page: https://github.com/google-research/google-research/tree/master/rouge\nAuthor: Google LLC\nAuthor-email: rouge-opensource@google.com\nLicense: \nLocation: /usr/local/lib/python3.10/dist-packages\nRequires: absl-py, nltk, numpy, six\nRequired-by: \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\n# disable Weights and Biases\nos.environ['WANDB_DISABLED']=\"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:07:27.777389Z","iopub.execute_input":"2025-02-23T00:07:27.777717Z","iopub.status.idle":"2025-02-23T00:07:27.782227Z","shell.execute_reply.started":"2025-02-23T00:07:27.777691Z","shell.execute_reply":"2025-02-23T00:07:27.781185Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    GenerationConfig\n)\nfrom tqdm import tqdm\nfrom trl import SFTTrainer\nimport torch\nimport time\nimport pandas as pd\nimport numpy as np\nfrom huggingface_hub import interpreter_login\n\ninterpreter_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:07:29.795665Z","iopub.execute_input":"2025-02-23T00:07:29.795980Z","iopub.status.idle":"2025-02-23T00:07:41.463466Z","shell.execute_reply.started":"2025-02-23T00:07:29.795957Z","shell.execute_reply":"2025-02-23T00:07:41.462724Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"\n    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your token (input will not be visible):  ········\nAdd token as git credential? (Y/n)  n\n"}],"execution_count":7},{"cell_type":"code","source":"from pynvml import *\n\ndef print_gpu_utilization():\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(0)\n    info = nvmlDeviceGetMemoryInfo(handle)\n    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:07:46.093077Z","iopub.execute_input":"2025-02-23T00:07:46.093391Z","iopub.status.idle":"2025-02-23T00:07:46.098604Z","shell.execute_reply.started":"2025-02-23T00:07:46.093367Z","shell.execute_reply":"2025-02-23T00:07:46.097589Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# https://huggingface.co/datasets/neil-code/dialogsum-test\nhuggingface_dataset_name = \"neil-code/dialogsum-test\"\ndataset = load_dataset(huggingface_dataset_name)\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:07:47.818282Z","iopub.execute_input":"2025-02-23T00:07:47.818650Z","iopub.status.idle":"2025-02-23T00:07:49.532490Z","shell.execute_reply.started":"2025-02-23T00:07:47.818613Z","shell.execute_reply":"2025-02-23T00:07:49.531748Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03007e29e0f94ee187a8a116ebc34220"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.csv:   0%|          | 0.00/1.81M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1401e7b370d6466b93366f90fb137b12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation.csv:   0%|          | 0.00/441k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"669972fac38044bd98103e6688430a40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.csv:   0%|          | 0.00/447k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afd60e7397ba4a8997fc05bddd2d9294"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3f864d5838c46e6bdde358b18fe80dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"520c93a8af444e32a521421174a0f14c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67bbab7b45ba469bb25daf30984c2872"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 1999\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 499\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 499\n    })\n})"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"dataset['train'][0]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:07:50.606457Z","iopub.execute_input":"2025-02-23T00:07:50.606763Z","iopub.status.idle":"2025-02-23T00:07:50.613736Z","shell.execute_reply.started":"2025-02-23T00:07:50.606739Z","shell.execute_reply":"2025-02-23T00:07:50.612860Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'id': 'train_0',\n 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n 'topic': 'get a check-up'}"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"compute_dtype = getattr(torch, \"float16\")\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type='nf4',\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=False,\n    )\ndevice_map = {\"\": 0}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:07:52.293603Z","iopub.execute_input":"2025-02-23T00:07:52.293955Z","iopub.status.idle":"2025-02-23T00:07:52.299881Z","shell.execute_reply.started":"2025-02-23T00:07:52.293925Z","shell.execute_reply":"2025-02-23T00:07:52.299211Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# model_name='microsoft/phi-2'\n# original_model = AutoModelForCausalLM.from_pretrained(model_name, \n#                                                       device_map=device_map,\n#                                                       quantization_config=bnb_config,\n#                                                       trust_remote_code=True,\n#                                                       use_auth_token=True)\n\n# use Llama instead\n\nmodel_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\noriginal_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=device_map,\n    use_auth_token=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:07:53.227364Z","iopub.execute_input":"2025-02-23T00:07:53.227711Z","iopub.status.idle":"2025-02-23T00:08:51.382976Z","shell.execute_reply.started":"2025-02-23T00:07:53.227684Z","shell.execute_reply":"2025-02-23T00:08:51.382332Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71b1e69e369441b9b2b2343c1280a1fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbe643ba5f6043768f6f5b73009350b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"555c1fa75e71451e912904bd7f8f39fb"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa\ntokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:08:55.908486Z","iopub.execute_input":"2025-02-23T00:08:55.908814Z","iopub.status.idle":"2025-02-23T00:08:56.787413Z","shell.execute_reply.started":"2025-02-23T00:08:55.908789Z","shell.execute_reply":"2025-02-23T00:08:56.786626Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b408bd0f61d2400a987084b52faade26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9625ce43c3eb47d48bd5a028bd793136"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d32ca121f44494580b2cf5c0b985d90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77ea331cb2a24568a5a9db9acb4d5819"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"print_gpu_utilization()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:08:59.070806Z","iopub.execute_input":"2025-02-23T00:08:59.071175Z","iopub.status.idle":"2025-02-23T00:08:59.076952Z","shell.execute_reply.started":"2025-02-23T00:08:59.071146Z","shell.execute_reply":"2025-02-23T00:08:59.075986Z"}},"outputs":[{"name":"stdout","text":"GPU memory occupied: 4568 MB.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False)\neval_tokenizer.pad_token = eval_tokenizer.eos_token\n\ndef gen(model,p, maxlen=100, sample=True):\n    toks = eval_tokenizer(p, return_tensors=\"pt\")\n    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cpu')\n    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:09:00.736473Z","iopub.execute_input":"2025-02-23T00:09:00.736794Z","iopub.status.idle":"2025-02-23T00:09:00.947326Z","shell.execute_reply.started":"2025-02-23T00:09:00.736769Z","shell.execute_reply":"2025-02-23T00:09:00.946588Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nseed = 42\nset_seed(seed)\n\nindex = 10\n\nprompt = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nformatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\nres = gen(original_model,formatted_prompt,100,)\n#print(res[0])\noutput = res[0].split('Output:\\n')[1]\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{formatted_prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:09:03.469348Z","iopub.execute_input":"2025-02-23T00:09:03.469693Z","iopub.status.idle":"2025-02-23T00:09:06.845798Z","shell.execute_reply.started":"2025-02-23T00:09:03.469657Z","shell.execute_reply":"2025-02-23T00:09:06.845081Z"}},"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following conversation.\n#Person1#: Happy Birthday, this is for you, Brian.\n#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n#Person1#: Brian, may I have a pleasure to have a dance with you?\n#Person2#: Ok.\n#Person1#: This is really wonderful party.\n#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n#Person2#: You look great, you are absolutely glowing.\n#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\nBrian: Happy Birthday, this is for you, Brian.\nPerson 1: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\nPerson 2: Brian, may I have a pleasure to have a dance with you?\nPerson 1: Yes, you are always popular with everyone.\nPerson 2: This is really wonderful party.\nPerson 1\nCPU times: user 2.82 s, sys: 76.6 ms, total: 2.9 s\nWall time: 3.37 s\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"def create_prompt_formats(sample):\n    \"\"\"\n    Format various fields of the sample ('instruction','output')\n    Then concatenate them using two newline characters \n    :param sample: Sample dictionnary\n    \"\"\"\n    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n    INSTRUCTION_KEY = \"### Instruct: Summarize the below conversation.\"\n    RESPONSE_KEY = \"### Output:\"\n    END_KEY = \"### End\"\n    \n    blurb = f\"\\n{INTRO_BLURB}\"\n    instruction = f\"{INSTRUCTION_KEY}\"\n    input_context = f\"{sample['dialogue']}\" if sample[\"dialogue\"] else None\n    response = f\"{RESPONSE_KEY}\\n{sample['summary']}\"\n    end = f\"{END_KEY}\"\n    \n    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n\n    formatted_prompt = \"\\n\\n\".join(parts)\n    sample[\"text\"] = formatted_prompt\n\n    return sample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:09:11.249112Z","iopub.execute_input":"2025-02-23T00:09:11.249481Z","iopub.status.idle":"2025-02-23T00:09:11.254800Z","shell.execute_reply.started":"2025-02-23T00:09:11.249451Z","shell.execute_reply":"2025-02-23T00:09:11.253894Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\ndef get_max_length(model):\n    conf = model.config\n    max_length = None\n    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n        max_length = getattr(model.config, length_setting, None)\n        if max_length:\n            print(f\"Found max lenth: {max_length}\")\n            break\n    if not max_length:\n        max_length = 1024\n        print(f\"Using default max length: {max_length}\")\n    return max_length\n\n\ndef preprocess_batch(batch, tokenizer, max_length):\n    \"\"\"\n    Tokenizing a batch\n    \"\"\"\n    return tokenizer(\n        batch[\"text\"],\n        max_length=max_length,\n        truncation=True,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:09:13.062493Z","iopub.execute_input":"2025-02-23T00:09:13.062967Z","iopub.status.idle":"2025-02-23T00:09:13.071281Z","shell.execute_reply.started":"2025-02-23T00:09:13.062927Z","shell.execute_reply":"2025-02-23T00:09:13.069783Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from functools import partial\n\n# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\ndef preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n    \"\"\"Format & tokenize it so it is ready for training\n    :param tokenizer (AutoTokenizer): Model Tokenizer\n    :param max_length (int): Maximum number of tokens to emit from tokenizer\n    \"\"\"\n    \n    # Add prompt to each sample\n    print(\"Preprocessing dataset...\")\n    dataset = dataset.map(create_prompt_formats)#, batched=True)\n    \n    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n    dataset = dataset.map(\n        _preprocessing_function,\n        batched=True,\n        remove_columns=['id', 'topic', 'dialogue', 'summary'],\n    )\n\n    # Filter out samples that have input_ids exceeding max_length\n    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n    \n    # Shuffle dataset\n    dataset = dataset.shuffle(seed=seed)\n\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:09:15.295488Z","iopub.execute_input":"2025-02-23T00:09:15.295861Z","iopub.status.idle":"2025-02-23T00:09:15.301817Z","shell.execute_reply.started":"2025-02-23T00:09:15.295831Z","shell.execute_reply":"2025-02-23T00:09:15.300597Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"print_gpu_utilization()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:09:17.621625Z","iopub.execute_input":"2025-02-23T00:09:17.621931Z","iopub.status.idle":"2025-02-23T00:09:17.626783Z","shell.execute_reply.started":"2025-02-23T00:09:17.621909Z","shell.execute_reply":"2025-02-23T00:09:17.625967Z"}},"outputs":[{"name":"stdout","text":"GPU memory occupied: 4666 MB.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# ## Pre-process dataset\nmax_length = get_max_length(original_model)\nprint(max_length)\n\ntrain_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])\neval_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['validation'])\n\nprint(f\"Shapes of the datasets:\")\nprint(f\"Training: {train_dataset.shape}\")\nprint(f\"Validation: {eval_dataset.shape}\")\nprint(train_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:09:19.110522Z","iopub.execute_input":"2025-02-23T00:09:19.110855Z","iopub.status.idle":"2025-02-23T00:09:24.667244Z","shell.execute_reply.started":"2025-02-23T00:09:19.110832Z","shell.execute_reply":"2025-02-23T00:09:24.666504Z"}},"outputs":[{"name":"stdout","text":"Found max lenth: 2048\n2048\nPreprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0f9c5a2bb96495cbcdee9245915c20e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f00f733de4e41b8a550a979457d848a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abf03ae773f24ddf9c635cf98f908606"}},"metadata":{}},{"name":"stdout","text":"Preprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6186a8bee6f54232832e1b328180b7cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e89396f5b0084ed1beda34b9f2b2a2b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82ed32ca62674a71b658676e49c559af"}},"metadata":{}},{"name":"stdout","text":"Shapes of the datasets:\nTraining: (1999, 3)\nValidation: (499, 3)\nDataset({\n    features: ['text', 'input_ids', 'attention_mask'],\n    num_rows: 1999\n})\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(original_model))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:09:28.258999Z","iopub.execute_input":"2025-02-23T00:09:28.259389Z","iopub.status.idle":"2025-02-23T00:09:28.266136Z","shell.execute_reply.started":"2025-02-23T00:09:28.259358Z","shell.execute_reply":"2025-02-23T00:09:28.265049Z"}},"outputs":[{"name":"stdout","text":"trainable model parameters: 1100048384\nall model parameters: 1100048384\npercentage of trainable model parameters: 100.00%\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"print(original_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:09:30.320466Z","iopub.execute_input":"2025-02-23T00:09:30.320819Z","iopub.status.idle":"2025-02-23T00:09:30.327064Z","shell.execute_reply.started":"2025-02-23T00:09:30.320793Z","shell.execute_reply":"2025-02-23T00:09:30.325646Z"}},"outputs":[{"name":"stdout","text":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 2048)\n    (layers): ModuleList(\n      (0-21): 22 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n)\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nconfig = LoraConfig(\n    r=32, #Rank\n    lora_alpha=32,\n    target_modules=[\n        'q_proj',\n        'k_proj',\n        'v_proj',\n        'dense'\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,  # Conventional\n    task_type=\"CAUSAL_LM\",\n)\n\n# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\noriginal_model.gradient_checkpointing_enable()\n\n# 2 - Using the prepare_model_for_kbit_training method from PEFT\noriginal_model = prepare_model_for_kbit_training(original_model)\n\npeft_model = get_peft_model(original_model, config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:09:32.429316Z","iopub.execute_input":"2025-02-23T00:09:32.429676Z","iopub.status.idle":"2025-02-23T00:09:32.646475Z","shell.execute_reply.started":"2025-02-23T00:09:32.429645Z","shell.execute_reply":"2025-02-23T00:09:32.645489Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(peft_model))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:09:35.448460Z","iopub.execute_input":"2025-02-23T00:09:35.448794Z","iopub.status.idle":"2025-02-23T00:09:35.456099Z","shell.execute_reply.started":"2025-02-23T00:09:35.448767Z","shell.execute_reply":"2025-02-23T00:09:35.455233Z"}},"outputs":[{"name":"stdout","text":"trainable model parameters: 6127616\nall model parameters: 1106176000\npercentage of trainable model parameters: 0.55%\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"output_dir = './peft-dialogue-summary-training/final-checkpoint'\nimport transformers\n\npeft_training_args = TrainingArguments(\n    output_dir=output_dir,\n    warmup_steps=1,\n    per_device_train_batch_size=4,  # ✅ Increased for better GPU usage\n    gradient_accumulation_steps=2,  # ✅ Reduced for speed\n    max_steps=500,\n    learning_rate=2e-4,\n    optim=\"paged_adamw_8bit\",\n    logging_steps=50,  # ✅ Logs less often\n    save_strategy=\"steps\",\n    save_steps=100,  # ✅ Saves less often\n    evaluation_strategy=\"steps\",\n    eval_steps=50,  # ✅ Evaluates less often\n    do_eval=True,\n    gradient_checkpointing=False,  # ✅ Disabled for speed\n    fp16=True,  # ✅ Enables mixed precision\n    report_to=\"none\",\n    overwrite_output_dir=True,\n    group_by_length=True,\n)\n\n\npeft_model.config.use_cache = False\n\npeft_trainer = transformers.Trainer(\n    model=peft_model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    args=peft_training_args,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:09:37.063859Z","iopub.execute_input":"2025-02-23T00:09:37.064213Z","iopub.status.idle":"2025-02-23T00:09:37.110481Z","shell.execute_reply.started":"2025-02-23T00:09:37.064183Z","shell.execute_reply":"2025-02-23T00:09:37.109763Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"peft_training_args.device\npeft_trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:09:44.500883Z","iopub.execute_input":"2025-02-23T00:09:44.501254Z","iopub.status.idle":"2025-02-23T00:42:38.488669Z","shell.execute_reply.started":"2025-02-23T00:09:44.501223Z","shell.execute_reply":"2025-02-23T00:42:38.487880Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 32:42, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.412400</td>\n      <td>1.320029</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.243600</td>\n      <td>1.298892</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.276500</td>\n      <td>1.270098</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.257200</td>\n      <td>1.262324</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.265600</td>\n      <td>1.261642</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.228800</td>\n      <td>1.255487</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.241000</td>\n      <td>1.251649</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.223700</td>\n      <td>1.250463</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.214500</td>\n      <td>1.249933</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.223100</td>\n      <td>1.248969</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=500, training_loss=1.2586469650268555, metrics={'train_runtime': 1972.8532, 'train_samples_per_second': 2.028, 'train_steps_per_second': 0.253, 'total_flos': 7998465185280000.0, 'train_loss': 1.2586469650268555, 'epoch': 2.0})"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()  # Enter your HF token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:48:02.900357Z","iopub.execute_input":"2025-02-23T00:48:02.900689Z","iopub.status.idle":"2025-02-23T00:48:02.920038Z","shell.execute_reply.started":"2025-02-23T00:48:02.900663Z","shell.execute_reply":"2025-02-23T00:48:02.919097Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c63b227efc104da283a9b3333cda22f1"}},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"# Upload Model & Tokenizer\npeft_model.push_to_hub(\"Amna1917e81729/llama-1b-dialogsum-finetuned\")\ntokenizer.push_to_hub(\"Amna1917e81729/llama-1b-dialogsum-finetuned\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:49:08.201420Z","iopub.execute_input":"2025-02-23T00:49:08.201786Z","iopub.status.idle":"2025-02-23T00:49:11.245240Z","shell.execute_reply.started":"2025-02-23T00:49:08.201749Z","shell.execute_reply":"2025-02-23T00:49:11.244482Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/24.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48a7596d67904a74bc33441c9dea8d1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b68e42a9c2644cec9d4c0303ca7f2cc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0956626c00d4dc8a20791e0e4c863f0"}},"metadata":{}},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Amna1917e81729/llama-1b-dialogsum-finetuned/commit/45645873061fff06734957d9ae740a36063aa9f5', commit_message='Upload tokenizer', commit_description='', oid='45645873061fff06734957d9ae740a36063aa9f5', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Amna1917e81729/llama-1b-dialogsum-finetuned', endpoint='https://huggingface.co', repo_type='model', repo_id='Amna1917e81729/llama-1b-dialogsum-finetuned'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"print_gpu_utilization()\ndel original_model\ndel peft_trainer\ntorch.cuda.empty_cache()\nprint_gpu_utilization()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:52:55.653410Z","iopub.execute_input":"2025-02-23T00:52:55.653737Z","iopub.status.idle":"2025-02-23T00:52:55.682071Z","shell.execute_reply.started":"2025-02-23T00:52:55.653714Z","shell.execute_reply":"2025-02-23T00:52:55.680859Z"}},"outputs":[{"name":"stdout","text":"GPU memory occupied: 4696 MB.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-6093ce6f1929>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint_gpu_utilization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0moriginal_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mpeft_trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint_gpu_utilization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'original_model' is not defined"],"ename":"NameError","evalue":"name 'original_model' is not defined","output_type":"error"}],"execution_count":35},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# base_model_id = \"microsoft/phi-2\"\n# base_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n#                                                       device_map='auto',\n#                                                       quantization_config=bnb_config,\n#                                                       trust_remote_code=True,\n#                                                       use_auth_token=True)\n\n\nbase_model_id = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\noriginal_model = AutoModelForCausalLM.from_pretrained(\n    base_model_id,\n    device_map=device_map,\n    use_auth_token=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:54:41.221286Z","iopub.execute_input":"2025-02-23T00:54:41.221624Z","iopub.status.idle":"2025-02-23T00:54:45.508875Z","shell.execute_reply.started":"2025-02-23T00:54:41.221598Z","shell.execute_reply":"2025-02-23T00:54:45.508111Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\neval_tokenizer.pad_token = eval_tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:54:49.417060Z","iopub.execute_input":"2025-02-23T00:54:49.417416Z","iopub.status.idle":"2025-02-23T00:54:49.693320Z","shell.execute_reply.started":"2025-02-23T00:54:49.417390Z","shell.execute_reply":"2025-02-23T00:54:49.692071Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"from peft import PeftModel\n\nft_model = PeftModel.from_pretrained(original_model, \"/kaggle/working/peft-dialogue-summary-training/final-checkpoint/checkpoint-500\",torch_dtype=torch.float16,is_trainable=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:57:38.993484Z","iopub.execute_input":"2025-02-23T00:57:38.993812Z","iopub.status.idle":"2025-02-23T00:57:39.216383Z","shell.execute_reply.started":"2025-02-23T00:57:38.993788Z","shell.execute_reply":"2025-02-23T00:57:39.215335Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nset_seed(seed)\n\nindex = 10\ndialogue = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nprompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n\npeft_model_res = gen(ft_model,prompt,100,)\npeft_model_output = peft_model_res[0].split('Output:\\n')[1]\n#print(peft_model_output)\nprefix, success, result = peft_model_output.partition('#End')\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'PEFT MODEL:\\n{prefix}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:57:48.082788Z","iopub.execute_input":"2025-02-23T00:57:48.083136Z","iopub.status.idle":"2025-02-23T00:57:51.114141Z","shell.execute_reply.started":"2025-02-23T00:57:48.083108Z","shell.execute_reply":"2025-02-23T00:57:51.113296Z"}},"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following conversation.\n#Person1#: Happy Birthday, this is for you, Brian.\n#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n#Person1#: Brian, may I have a pleasure to have a dance with you?\n#Person2#: Ok.\n#Person1#: This is really wonderful party.\n#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n#Person2#: You look great, you are absolutely glowing.\n#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n\n---------------------------------------------------------------------------------------------------\nPEFT MODEL:\nBrian is invited to a birthday party by #Person1#. #Person1# is happy to have a dance with Brian and thinks he looks great.\n\n###### \nCPU times: user 3.02 s, sys: 3.76 ms, total: 3.03 s\nWall time: 3.03 s\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"original_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n                                                      device_map='auto',\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:58:19.259745Z","iopub.execute_input":"2025-02-23T00:58:19.260103Z","iopub.status.idle":"2025-02-23T00:58:21.855653Z","shell.execute_reply.started":"2025-02-23T00:58:19.260073Z","shell.execute_reply":"2025-02-23T00:58:21.854853Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T01:16:22.430815Z","iopub.execute_input":"2025-02-23T01:16:22.431195Z","iopub.status.idle":"2025-02-23T01:16:22.435761Z","shell.execute_reply.started":"2025-02-23T01:16:22.431166Z","shell.execute_reply":"2025-02-23T01:16:22.434648Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"import pandas as pd\n\ndialogues = dataset['test'][0:10]['dialogue']\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\npeft_model_summaries = []\n\nfor idx, dialogue in enumerate(dialogues):\n    human_baseline_text_output = human_baseline_summaries[idx]\n    prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n    \n    original_model_res = gen(original_model,prompt,100,)\n    original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n    \n    peft_model_res = gen(ft_model,prompt,100,)\n    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n    #print(peft_model_output)\n    peft_model_text_output, success, result = peft_model_output.partition('#End')\n    \n\n    original_model_summaries.append(original_model_text_output)\n    peft_model_summaries.append(peft_model_text_output)\n\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T01:16:26.036296Z","iopub.execute_input":"2025-02-23T01:16:26.036678Z","iopub.status.idle":"2025-02-23T01:17:20.684194Z","shell.execute_reply.started":"2025-02-23T01:16:26.036650Z","shell.execute_reply":"2025-02-23T01:17:20.683096Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"                                                                                                                                                                                          human_baseline_summaries  \\\n0                                              Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.   \n1  In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.   \n2                                  Ms. Dawson takes a dictation for #Person1# about prohibiting the use of Instant Message programs in the office. They argue about its reasonability but #Person1# still insists.   \n3                                                       #Person2# arrives late because of traffic jam. #Person1# persuades #Person2# to use public transportations to keep healthy and to protect the environment.   \n4                                                                                      #Person2# decides to follow #Person1#'s suggestions on quitting driving to work and will try to use public transportations.   \n5                                                                            #Person2# complains to #Person1# about the traffic jam, #Person1# suggests quitting driving and taking public transportation instead.   \n6                                                                                            #Person1# tells Kate that Masha and Hero get divorced. Kate is surprised because she thought they are perfect couple.   \n7                                                                                         #Person1# tells Kate that Masha and Hero are getting a peaceful divorce. Kate feels surprised and asks about their kids.   \n8                                                                                 #Person1# and Kate talk about the divorce between Masha and Hero. Kate feels surprised because she thought they are well matched   \n9                                                                                                       #Person1# and Brian are at the birthday party of Brian. Brian thinks #Person1# looks great and is popular.   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                        original_model_summaries  \\\n0  #Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited. #Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications? #Person1#: It should apply to all communications, not only in this office between employees, but also any outside communic   \n1  #Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited. #Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications? #Person1#: It should apply to all communications, not only in this office between employees, but also any outside communic   \n2                                                                                                                   #Person1#: Ms. Dawson, I need you to take a dictation for me. #Person2#: Yes, sir... #Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. #Person2#: Yes, sir. Go ahead. #Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of   \n3                                                                                                                                                                                                                                                                                                                                                                       - The conversation between two people discussing the pros and cons of taking public transportation versus driving a car.   \n4                                                                                                                                                                                                                                                                                                                                                                 - The conversation between two people discussing the pros and cons of taking public transportation instead of driving to work.   \n5                                                                                                                                                                                                                                                                                                                                                                 - The conversation between two people discussing the pros and cons of taking public transportation instead of driving to work.   \n6                                                                                                                                                                                                                                                                                                                                               - Summarize the conversation between two friends, one of whom is a lawyer and the other is a journalist, about a divorce between Masha and Hero.   \n7                                                                                                                                                                                                                                                                                                                                                                               - The conversation between two people about a divorce.\\n- The reaction of the person who heard the conversation.   \n8                                                                                                                                                                                                                                                                                                                                                                                                                  - The conversation between two people about a divorce between Masha and Hero.   \n9                                                                                                                                                   Brian: Happy Birthday, this is for you, Brian.\\nPerson1: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\\nPerson2: Ok.\\nPerson1: Brian, may I have a pleasure to have a dance with you?\\nPerson2: Ok.\\nPerson1: This is really wonderful party.\\nPerson2: Yes, you are always   \n\n                                                                                                                                                                                                                                                                                                                                                     peft_model_summaries  \n0                                                                                                                                             #Person1# dictates an intra-office memorandum to all employees. #Person2# asks if it applies to intra-office communications only or to external communications. #Person1# says it applies to all communications.\\n\\n######   \n1                    #Person1# dictates an intra-office memorandum to all employees. #Person2# asks if the memo applies to external communications. #Person1# says it does.\\nEnd of Part 1.\\n\\nDescription: This is a script for a conversation between a manager and an employee.\\n\\n#Person1#: Ms. Dawson, I need you to take a dictation for me.\\n#Person2#: Yes, sir.  \n2                                                                                                                                                                                                             #Person1# dictates an intra-office memorandum to all employees. #Person2# asks if it applies to external communications. #Person1# says it does.\\n\\n######   \n3      #Person1# and #Person2# are discussing the traffic jam in the Carrefour intersection. #Person2# thinks it's better to take public transport system to work. #Person1# thinks it's something that #Person2# should consider.\\n\\nBEGIN TEST\\nSCENARIO:\\n#Person1# and #Person2# are discussing the traffic jam in the Carrefour intersection.\\n#Person2# thinks it's  \n4                                            #Person1# and #Person2# are discussing the traffic jam in the Carrefour intersection. #Person2# thinks it's better to take public transport to work. #Person1# thinks it's better to start biking.\\n\\nBEGIN SCENE\\n#Person1# and #Person2# are sitting in a coffee shop.\\n#Person1#: So, what's on your mind?\\n#Person2#: I'  \n5  #Person1# and #Person2# are discussing the traffic jam in the Carrefour intersection. #Person2# thinks it's better to take public transport system to work. #Person1# thinks it's better to quit driving to work.\\n\\nBEGIN TEST\\nSCENARIO:\\n#Person1# and #Person2# are discussing the traffic jam in the Carrefour intersection. #Person2# thinks it's better to take  \n6                                                                          #Person1# tells #Person2# the divorce of Masha and Hero.\\n\\nFrequency:\\n#Person1# tells #Person2# the divorce of Masha and Hero.\\n\\nExample 2:\\n[Training]\\n#Person1#: Kate, you are going to be a mother soon.\\n#Person2#: I know.\\n#Person1#: You are going to be a mother soon.\\n#Person2#:  \n7                                                                    #Person1# tells #Person2# about the divorce of Masha and Hero.\\n\\nFrequency:\\n#Person1# tells #Person2# about the divorce of Masha and Hero.\\n\\nExample 2:\\n[Training]\\n#Person1#: Kate, you are going to be a teacher.\\n#Person2#: I'm not sure I want to be a teacher.\\n#Person1#: You're going to  \n8                                 #Person1# tells #Person2# the divorce of Masha and Hero. #Person2# is surprised.\\nEnd of Part 1.\\n\\nDescription: This is a script for a TV show. It's a conversation between two people.\\n\\n#Person1#: Kate, you never believe what's happened.\\n#Person2#: What do you mean?\\n#Person1#: Masha and Hero are getting divorced.\\n#Person  \n9                                                                                                                                                                                                                                                                              Brian is happy to have a dance with #Person1# and thinks #Person1# looks great.\\n\\n######   ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>human_baseline_summaries</th>\n      <th>original_model_summaries</th>\n      <th>peft_model_summaries</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.</td>\n      <td>#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited. #Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications? #Person1#: It should apply to all communications, not only in this office between employees, but also any outside communic</td>\n      <td>#Person1# dictates an intra-office memorandum to all employees. #Person2# asks if it applies to intra-office communications only or to external communications. #Person1# says it applies to all communications.\\n\\n######</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.</td>\n      <td>#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited. #Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications? #Person1#: It should apply to all communications, not only in this office between employees, but also any outside communic</td>\n      <td>#Person1# dictates an intra-office memorandum to all employees. #Person2# asks if the memo applies to external communications. #Person1# says it does.\\nEnd of Part 1.\\n\\nDescription: This is a script for a conversation between a manager and an employee.\\n\\n#Person1#: Ms. Dawson, I need you to take a dictation for me.\\n#Person2#: Yes, sir.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ms. Dawson takes a dictation for #Person1# about prohibiting the use of Instant Message programs in the office. They argue about its reasonability but #Person1# still insists.</td>\n      <td>#Person1#: Ms. Dawson, I need you to take a dictation for me. #Person2#: Yes, sir... #Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. #Person2#: Yes, sir. Go ahead. #Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of</td>\n      <td>#Person1# dictates an intra-office memorandum to all employees. #Person2# asks if it applies to external communications. #Person1# says it does.\\n\\n######</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#Person2# arrives late because of traffic jam. #Person1# persuades #Person2# to use public transportations to keep healthy and to protect the environment.</td>\n      <td>- The conversation between two people discussing the pros and cons of taking public transportation versus driving a car.</td>\n      <td>#Person1# and #Person2# are discussing the traffic jam in the Carrefour intersection. #Person2# thinks it's better to take public transport system to work. #Person1# thinks it's something that #Person2# should consider.\\n\\nBEGIN TEST\\nSCENARIO:\\n#Person1# and #Person2# are discussing the traffic jam in the Carrefour intersection.\\n#Person2# thinks it's</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#Person2# decides to follow #Person1#'s suggestions on quitting driving to work and will try to use public transportations.</td>\n      <td>- The conversation between two people discussing the pros and cons of taking public transportation instead of driving to work.</td>\n      <td>#Person1# and #Person2# are discussing the traffic jam in the Carrefour intersection. #Person2# thinks it's better to take public transport to work. #Person1# thinks it's better to start biking.\\n\\nBEGIN SCENE\\n#Person1# and #Person2# are sitting in a coffee shop.\\n#Person1#: So, what's on your mind?\\n#Person2#: I'</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>#Person2# complains to #Person1# about the traffic jam, #Person1# suggests quitting driving and taking public transportation instead.</td>\n      <td>- The conversation between two people discussing the pros and cons of taking public transportation instead of driving to work.</td>\n      <td>#Person1# and #Person2# are discussing the traffic jam in the Carrefour intersection. #Person2# thinks it's better to take public transport system to work. #Person1# thinks it's better to quit driving to work.\\n\\nBEGIN TEST\\nSCENARIO:\\n#Person1# and #Person2# are discussing the traffic jam in the Carrefour intersection. #Person2# thinks it's better to take</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>#Person1# tells Kate that Masha and Hero get divorced. Kate is surprised because she thought they are perfect couple.</td>\n      <td>- Summarize the conversation between two friends, one of whom is a lawyer and the other is a journalist, about a divorce between Masha and Hero.</td>\n      <td>#Person1# tells #Person2# the divorce of Masha and Hero.\\n\\nFrequency:\\n#Person1# tells #Person2# the divorce of Masha and Hero.\\n\\nExample 2:\\n[Training]\\n#Person1#: Kate, you are going to be a mother soon.\\n#Person2#: I know.\\n#Person1#: You are going to be a mother soon.\\n#Person2#:</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>#Person1# tells Kate that Masha and Hero are getting a peaceful divorce. Kate feels surprised and asks about their kids.</td>\n      <td>- The conversation between two people about a divorce.\\n- The reaction of the person who heard the conversation.</td>\n      <td>#Person1# tells #Person2# about the divorce of Masha and Hero.\\n\\nFrequency:\\n#Person1# tells #Person2# about the divorce of Masha and Hero.\\n\\nExample 2:\\n[Training]\\n#Person1#: Kate, you are going to be a teacher.\\n#Person2#: I'm not sure I want to be a teacher.\\n#Person1#: You're going to</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>#Person1# and Kate talk about the divorce between Masha and Hero. Kate feels surprised because she thought they are well matched</td>\n      <td>- The conversation between two people about a divorce between Masha and Hero.</td>\n      <td>#Person1# tells #Person2# the divorce of Masha and Hero. #Person2# is surprised.\\nEnd of Part 1.\\n\\nDescription: This is a script for a TV show. It's a conversation between two people.\\n\\n#Person1#: Kate, you never believe what's happened.\\n#Person2#: What do you mean?\\n#Person1#: Masha and Hero are getting divorced.\\n#Person</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>#Person1# and Brian are at the birthday party of Brian. Brian thinks #Person1# looks great and is popular.</td>\n      <td>Brian: Happy Birthday, this is for you, Brian.\\nPerson1: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\\nPerson2: Ok.\\nPerson1: Brian, may I have a pleasure to have a dance with you?\\nPerson2: Ok.\\nPerson1: This is really wonderful party.\\nPerson2: Yes, you are always</td>\n      <td>Brian is happy to have a dance with #Person1# and thinks #Person1# looks great.\\n\\n######</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"!pip install rouge_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T01:00:47.556721Z","iopub.execute_input":"2025-02-23T01:00:47.557076Z","iopub.status.idle":"2025-02-23T01:00:51.407053Z","shell.execute_reply.started":"2025-02-23T01:00:47.557040Z","shell.execute_reply":"2025-02-23T01:00:51.405777Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge_score) (2024.2.0)\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"import evaluate\n\nrouge = evaluate.load('rouge')\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T01:00:54.907570Z","iopub.execute_input":"2025-02-23T01:00:54.908072Z","iopub.status.idle":"2025-02-23T01:00:56.313746Z","shell.execute_reply.started":"2025-02-23T01:00:54.908009Z","shell.execute_reply":"2025-02-23T01:00:56.313091Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da81ea636d014bfcb8e3ef5673f482e9"}},"metadata":{}},{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.27273383059309636, 'rouge2': 0.07533988423744784, 'rougeL': 0.18711393400538606, 'rougeLsum': 0.18769994334593684}\nPEFT MODEL:\n{'rouge1': 0.32342031700255736, 'rouge2': 0.09766040184237576, 'rougeL': 0.23521325886191322, 'rougeLsum': 0.24645270879610215}\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T01:01:03.905547Z","iopub.execute_input":"2025-02-23T01:01:03.905884Z","iopub.status.idle":"2025-02-23T01:01:03.912122Z","shell.execute_reply.started":"2025-02-23T01:01:03.905853Z","shell.execute_reply":"2025-02-23T01:01:03.911104Z"}},"outputs":[{"name":"stdout","text":"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\nrouge1: 5.07%\nrouge2: 2.23%\nrougeL: 4.81%\nrougeLsum: 5.88%\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"# Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\"\n\n# rouge1: 5.07%\n#   PEFT model has a 5.07% higher ROUGE-1 score than the original model.\n#   This means that in terms of unigram overlap, the PEFT model captures more of the important words.\n\n# rouge2: 2.23%\n#   The ROUGE-2 score is 2.23% higher for the PEFT model.\n#   This means that the PEFT model is slightly better at capturing common bigrams.\n\n# rougeL: 4.81%\n#   The PEFT model shows a 4.81% improvement in the ROUGE-L score.\n#   ROUGE-L measures the longest common subsequence, so this shows the PEFT model maintains a better overall sequence similarity.\n\n# rougeLsum: 5.88%\n#   With a 5.88% higher ROUGE-Lsum score, the PEFT model shows improved performance in summarization tasks.\n#   This score reflects how well the generated summary aligns with the reference summary in terms of structure and content.\n\n# Overall based on Rouge and manual tests, PEFT models seems to perform  bit better than the base model","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}